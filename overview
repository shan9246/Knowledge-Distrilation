Knowledge distillation is a model compression technique in which a compact student model
learns to replicate the behavior of a larger, more complex teacher model. This approach is
particularly beneficial when deploying deep learning models on resource-constrained devices.

Traditional knowledge distillation techniques rely on labeled real data to transfer knowl-
edge. However, in scenarios where training data is unavailable due to privacy concerns, data

scarcity, or proprietary constraints, alternative strategies are required.
Data-Free Adversarial Knowledge Distillation (AKD) offers a compelling solution by
eliminating the need for real training data. In this setting, a generator network is trained
adversarially to produce synthetic inputs that effectively simulate real data. These synthetic
samples are then used to train the student model by minimizing the discrepancy between its
predictions and the teacher model’s soft targets.
In this assignment, we implement a data-free AKD pipeline using PyTorch. The teacher
model is a ResNet-34 pre-trained on CIFAR-100. Two student models—Student20 (with 20%
parameters) and Student50 (with 50% parameters)—are trained entirely on synthetic data
generated by the generator. The results highlight the feasibility and potential of data-free
learning frameworks.
